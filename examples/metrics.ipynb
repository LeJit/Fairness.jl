{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:13:39.844000+05:30",
     "start_time": "2020-07-22T11:42:57.013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Desktop/Fairness/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "\n",
    "using Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Fairness Tensor\n",
    "\n",
    "```fair_tensor``` function is used for this. It accepts 3 arguments:\n",
    " - ŷ : Predicted Class\n",
    " - y : Ground truth\n",
    " - grp : Protected attribute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:13:47.085000+05:30",
     "start_time": "2020-07-22T11:42:57.044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fairness.FairTensor{3}([1 0; 1 1; 0 0]\n",
       "\n",
       "[0 1; 0 0; 1 0], [\"African\", \"American\", \"Indian\"])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CategoricalArrays\n",
    "\n",
    "ŷ = categorical([1, 0, 1, 1, 0])\n",
    "y = categorical([1, 1, 0, 1, 0])\n",
    "grp = categorical([\"African\", \"American\", \"Indian\", \"American\", \"African\"])\n",
    "\n",
    "ft = fair_tensor(ŷ, y, grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall use a Toy dataset of jobs, containing 10 rows. ```job_fairtensor``` function from data.jl shall be used to get the fairness tensor. In our further discussion we shall be using this same fairness tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:13:58.968000+05:30",
     "start_time": "2020-07-22T11:42:57.051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2; 0 0; 0 2]\n",
      "\n",
      "[0 0; 2 1; 1 0]\n",
      "\n",
      "[\"Board\", \"Education\", \"Healthcare\"]\n"
     ]
    }
   ],
   "source": [
    "ft = @load_toyfairtensor\n",
    "\n",
    "println(ft.mat)\n",
    "println()\n",
    "println(ft.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "All the metrics are Callable structs. Upon instantiation of the metric, it should be called by passing the fairness tensor.\n",
    "\n",
    "Its general form is ```metric(ft::FairTensor; grp=nothing)```\n",
    "\n",
    "Metrics can have multiple aliases as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:13:59.337000+05:30",
     "start_time": "2020-07-22T11:42:57.059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333334\n",
      "0.3333333333333334\n",
      "0.3333333333333334\n",
      "0.3333333333333334\n",
      "0.3333333333333334\n"
     ]
    }
   ],
   "source": [
    "tp = TruePositiveRate()\n",
    "println(tp(ft))\n",
    "\n",
    "println(true_positive_rate(ft))\n",
    "println(truepositive_rate(ft))\n",
    "println(tpr(ft))\n",
    "println(TPR()(ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Specific Metric Calculation\n",
    "\n",
    "The metrics can be calculated for specific groups by passing the grp argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:13:59.521000+05:30",
     "start_time": "2020-07-22T11:42:57.067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000000000000001\n",
      "0.9999999999999994\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "println(true_positive_rate(ft; grp=\"Board\"))\n",
    "\n",
    "println(ppv(ft; grp=\"Education\"))\n",
    "\n",
    "println(TruePositive()(ft; grp=\"Healthcare\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Calc-Metrics\n",
    "\n",
    "These are the metrics that return numerical values. \n",
    "\n",
    "The optional `grp` argument can also be passed to them to get the group specific metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:13:59.791000+05:30",
     "start_time": "2020-07-22T11:42:57.076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive : 2\n",
      "True Negative : 1\n",
      "False Positive : 3\n",
      "False Negative : 4\n",
      "True Positive Rate : 0.3333333333333334\n",
      "True Negative Rate : 0.2500000000000002\n",
      "False Positive Rate : 0.7499999999999998\n",
      "False Negative :  Rate : 0.6666666666666665\n",
      "False Discovery Rate : 0.40000000000000013\n",
      "Positive Predictive Value : 0.5999999999999999\n",
      "Negative Predictive Value : 0.20000000000000018\n"
     ]
    }
   ],
   "source": [
    "println(\"True Positive : \", truepositive(ft))\n",
    "println(\"True Negative : \", truenegative(ft))\n",
    "println(\"False Positive : \", falsepositive(ft))\n",
    "println(\"False Negative : \", falsenegative(ft))\n",
    "println(\"True Positive Rate : \", truepositive_rate(ft))\n",
    "println(\"True Negative Rate : \", truenegative_rate(ft))\n",
    "println(\"False Positive Rate : \", falsepositive_rate(ft))\n",
    "println(\"False Negative :  Rate : \", falsenegative_rate(ft))\n",
    "println(\"False Discovery Rate : \", falsediscovery_rate(ft))\n",
    "println(\"Positive Predictive Value : \", positive_predictive_value(ft))\n",
    "println(\"Negative Predictive Value : \", negative_predictive_value(ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disparity\n",
    "\n",
    "`disparity(M, ft; refGrp=nothing)`\n",
    "\n",
    "M is the array of Fairness metrics, ft is Fairness Tensor and refGrp is the reference group.\n",
    "\n",
    "It computes disparity for fairness tensor `ft` with respect to an array of metrics `M` and returns a dataframe of disparity of these metrics.\n",
    "\n",
    "For any class A and a reference Group B, disparity = metric(A)/metric(B)\n",
    "\n",
    "Please note that division by 0 will result in NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:14:03.748000+05:30",
     "start_time": "2020-07-22T11:42:57.084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>labels</th><th>false_positive_rate_disparity</th><th>true_negative_rate_disparity</th><th>positive_predictive_value_disparity</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows × 5 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>Board</td><td>0.0</td><td>3.0</td><td>0.0</td></tr><tr><th>2</th><td>Education</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><th>3</th><td>Healthcare</td><td>1.5</td><td>3.0e-15</td><td>1.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& labels & false\\_positive\\_rate\\_disparity & true\\_negative\\_rate\\_disparity & positive\\_predictive\\_value\\_disparity & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & Board & 0.0 & 3.0 & 0.0 & $\\dots$ \\\\\n",
       "\t2 & Education & 1.0 & 1.0 & 1.0 & $\\dots$ \\\\\n",
       "\t3 & Healthcare & 1.5 & 3.0e-15 & 1.0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3×5 DataFrame. Omitted printing of 3 columns\n",
       "│ Row │ labels     │ false_positive_rate_disparity │\n",
       "│     │ \u001b[90mString\u001b[39m     │ \u001b[90mFloat64\u001b[39m                       │\n",
       "├─────┼────────────┼───────────────────────────────┤\n",
       "│ 1   │ Board      │ 0.0                           │\n",
       "│ 2   │ Education  │ 1.0                           │\n",
       "│ 3   │ Healthcare │ 1.5                           │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = [false_positive_rate, true_negative_rate, ppv, npv ]\n",
    "\n",
    "df = disparity(M, ft; refGrp=\"Education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Metrics\n",
    "\n",
    "These are the Metrics which return Boolean values.\n",
    "\n",
    "These metrics are callable structs. \n",
    "The struct has field for the ```A``` and ```C```. ```A``` corresponds to the matrix on LHS of the equality-check equation A*z = 0 in the paper https://arxiv.org/pdf/2004.03424.pdf Equation No. 3. In this paper it is a 1D array. But to deal with multiple group fairness, a 2D array matrix is used.\n",
    "\n",
    "Initially the instatiated metric contains ```0``` and ```[]``` as values for ```C``` and ```A```. But after calling it on fairness tensor, the values of ```C``` and ```A``` change as shown below. This gives the advantage to reuse the same instantiation again. But upon reusing, the matrix ```A``` need not be generated again as it will remain the same. This makes it faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:14:06.266000+05:30",
     "start_time": "2020-07-22T11:42:57.093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values in struct DemographicParity : \n",
      "A : Any[]\n",
      "C : 0\n",
      "\n",
      "Demographic Parity : false\n",
      "\n",
      "New values in dp (instance of DemographicParity)\n",
      "A : [4 0 4 0 -3 0 -3 0; 3 0 3 0 -3 0 -3 0; 0 0 0 0 0 0 0 0]\n",
      "C : 3\n"
     ]
    }
   ],
   "source": [
    "dp = DemographicParity()\n",
    "\n",
    "println(\"Initial values in struct DemographicParity : \")\n",
    "println(\"A : \", dp.A)\n",
    "println(\"C : \", dp.C)\n",
    "\n",
    "ft = @load_toyfairtensor\n",
    "\n",
    "println()\n",
    "println(\"Demographic Parity : \", dp(ft))\n",
    "println()\n",
    "\n",
    "println(\"New values in dp (instance of DemographicParity)\")\n",
    "println(\"A : \", dp.A)\n",
    "println(\"C : \", dp.C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
